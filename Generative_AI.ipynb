{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gersonvida12-hash/Desconstrutor-de-Prompts---Usa-top-tecnologias/blob/main/Generative_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17DEqM2HuLq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01878cf7-f4c8-426e-db7d-53a09203fc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%%writefile` not found.\n"
          ]
        }
      ],
      "source": [
        "# @phase_0: SETUP\n",
        "!pip install -q crewai python-dotenv langchain-groq\n",
        "!mkdir -p tools agents tasks\n",
        "!touch main.py agents.py tasks.py tools/c.py\n",
        "\n",
        "# @phase_1: WRITE ALL FILES\n",
        "%%writefile tools/c.py\n",
        "from crewai_tools import BaseTool as BT\n",
        "class CTool(BT):\n",
        "  name=\"ClarifyTool\";description=\"Ask user questions.\"\n",
        "  def _run(self,q:list[str])->str:\n",
        "    r=input(\"\\n\".join([f\"Q{i}:{s}\" for i,s in enumerate(q,1)]));return f\"User:{r}\"\n",
        "\n",
        "%%writefile agents.py\n",
        "from crewai import Agent as A;from tools.c import CTool\n",
        "def mk_a():\n",
        "  d=A(role='Diagnostician',goal='P0/1:Analyze,RCA',tools=[CTool()],allow_delegation=False)\n",
        "  r=A(role='RiskAnalyst',goal='P1:Pre-mortem',allow_delegation=False)\n",
        "  s=A(role='Architect',goal='P2:Synthesize solution',allow_delegation=False)\n",
        "  return d,r,s\n",
        "\n",
        "%%writefile tasks.py\n",
        "from crewai import Task as T\n",
        "def mk_t(ags,p):\n",
        "  d,r,s=ags\n",
        "  t1=T(description=f\"Analyze:{p}\",expected_output=\"JSON:vector,rca,context\",agent=d)\n",
        "  t2=T(description=\"Risk analysis\",expected_output=\"MD_TABLE:Risk Matrix\",agent=r,context=[t1])\n",
        "  t3=T(description=\"Build HYPERION_PACKAGE\",expected_output=\"MD_BLOCK:Final package\",agent=s,context=[t2])\n",
        "  return[t1,t2,t3]\n",
        "\n",
        "%%writefile main.py\n",
        "import os;from crewai import Crew,Process as P;from agents import mk_a;from tasks import mk_t;from langchain_groq import ChatGroq;from google.colab import userdata\n",
        "try:os.environ['GROQ_API_KEY']=userdata.get('GROQ_API_KEY')\n",
        "except:os.environ['GROQ_API_KEY']=input(\"GROQ_API_KEY?:\")\n",
        "llm=ChatGroq(model_name=\"llama3-70b-8192\")\n",
        "def run(p):\n",
        "  ags=mk_a();[setattr(a,'llm',llm) for a in ags];tasks=mk_t(ags,p)\n",
        "  crew=Crew(agents=list(ags),tasks=tasks,process=P.sequential)\n",
        "  return crew.kickoff()\n",
        "if __name__==\"__main__\":\n",
        "  p=input(\"PROBLEM?:\");res=run(p);print(f\"\\n---PACKAGE---\\n{res}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edbcd1ff"
      },
      "source": [
        "!python main.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a generative AI model involves several steps, from gathering data to deploying your model in the real world.\n",
        "\n",
        "**Step 1:** Gathering Data\n",
        "The very first step is to collect the right data for your project. Here’s how you can do it:\n",
        "\n",
        "\n",
        "*   **Data Selection**: Decide what you want your model to create, whether it’s stories, poems, or even responses in a chatbot.\n",
        "*   **Data Collection**: Find lots of examples of what you want to generate. For text, you can gather books, articles, or conversations from the internet.\n",
        "\n",
        "**Step 2:** Preprocessing Your Data\n",
        "Before feeding your data to the AI model, you need to prepare it:\n",
        "\n",
        "\n",
        "*   **Cleaning**: Remove any messy or irrelevant parts from your data.\n",
        "*   **Tokenization**: Split your text into smaller chunks, like words or sentences.\n",
        "*   **Normalization**: Ensure everything is consistent; for text, it means converting everything to lowercase.\n",
        "\n",
        "**Step 3**: Choosing a Generative Model Architecture\n",
        "Like 1. Recurrent Neural Networks (RNNs) 2. Generative Adversarial Networks (GANs)\n",
        "\n",
        "**Step 4**: Training Your Generative Model\n",
        "Training is where your AI model learns to be creative:\n",
        "\n",
        "\n",
        "*   **Batch Size**: Experiment with different batch sizes to find what works best for your model.\n",
        "*   **Training Time**: Be patient; training can take a while, especially with lots of data.\n",
        "*   **Regularization**: Use techniques like dropout to prevent your model from getting too obsessed with the training data.\n",
        "\n",
        "**Step 5**: Evaluating and Fine-Tuning\n",
        "After training, it’s time to assess your model’s performance and make it even better:\n",
        "\n",
        "\n",
        "*  **Evaluation Metrics**: Determine how to measure the quality of what your model generates. For text, you might use metrics like “how human-like is it?”\n",
        "*   **Fine-Tuning**: Based on your evaluation, tweak your model. Adjust settings, get more data, or change the architecture.\n",
        "\n",
        "**Step 6**: Deploying Your Generative Model\n",
        "Once your model is ready, it’s time to put it to work:\n",
        "\n",
        "\n",
        "*   **API Integration**: If you want others to use your model, create an API so they can interact with it easily.\n",
        "*   **Monitoring**: Keep an eye on how your model performs in the real world and update it when needed."
      ],
      "metadata": {
        "id": "XntgT7G-uk6v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ojORviSu8Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gOutFW0tux4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "8K4rtYmJ0hNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**: Gathering Data\n",
        "Let’s assume you have a list of sentences as your dataset:"
      ],
      "metadata": {
        "id": "nRc6WkxF0uWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"This is the first sentence.\", \"Here's the second sentence.\", \"Finally, the third one.\"]"
      ],
      "metadata": {
        "id": "JokIkVb_0x0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: Preprocessing Your Data"
      ],
      "metadata": {
        "id": "LQ5rQh_v03kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "input_sequences = []\n",
        "for sequence in sequences:\n",
        "    for i in range(1, len(sequence)):\n",
        "        n_gram_sequence = sequence[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "print(\"done with preprocessing data\")"
      ],
      "metadata": {
        "id": "8ixv38V905XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Choosing a Generative Model Architecture\n",
        "Let’s set up an RNN for text generation:"
      ],
      "metadata": {
        "id": "k7X-8pcK1YLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 64, input_length=max_sequence_length-1),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wtGocj9I1bqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**: Training Your Generative Model"
      ],
      "metadata": {
        "id": "0dHjvcQW1kkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "AaaHGq001n-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**: Generating Text"
      ],
      "metadata": {
        "id": "wdjPwMzg1v6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "seed_text = \"This is\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    # predicted = model.predict_classes(token_list, verbose=0)\n",
        "    # predicted = (model.predict(token_list) > 0.5).astype(\"int32\")\n",
        "    predicted = np.argmax(model.predict(token_list),axis=1)\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "T6pkp9JM1y-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}